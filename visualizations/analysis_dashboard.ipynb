{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Multilingual Safety Evaluation - Analysis Dashboard\n",
    "\n",
    "This notebook provides interactive visualizations and analysis for LLM safety evaluation results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load evaluation results\n",
    "# Replace with your actual results file\n",
    "results_file = '../results/evaluation_latest.csv'\n",
    "df = pd.read_csv(results_file)\n",
    "\n",
    "print(f\"Loaded {len(df)} evaluation results\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLanguages: {df['language'].unique()}\")\n",
    "print(f\"Domains: {df['domain'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Overall Safety Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate overall metrics\n",
    "overall_metrics = {\n",
    "    'Total Evaluations': len(df),\n",
    "    'Average Safety Score': df['safety_score'].mean(),\n",
    "    'Average Compliance Score': df['compliance_score'].mean(),\n",
    "    'Average Cultural Score': df['cultural_score'].mean(),\n",
    "    'Critical Failures': len(df[df['risk_level'] == 'CRITICAL']),\n",
    "    'High Risk Count': len(df[df['risk_level'] == 'HIGH']),\n",
    "    'Pass Rate': len(df[df['compliance_score'] >= 0.7]) / len(df) * 100\n",
    "}\n",
    "\n",
    "# Display metrics\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Indicator(\n",
    "    mode = \"number+gauge+delta\",\n",
    "    value = overall_metrics['Average Safety Score'],\n",
    "    domain = {'x': [0, 1], 'y': [0, 1]},\n",
    "    title = {'text': \"Overall Safety Score\"},\n",
    "    delta = {'reference': 80},\n",
    "    gauge = {\n",
    "        'axis': {'range': [None, 100]},\n",
    "        'bar': {'color': \"darkblue\"},\n",
    "        'steps': [\n",
    "            {'range': [0, 40], 'color': \"lightgray\"},\n",
    "            {'range': [40, 60], 'color': \"yellow\"},\n",
    "            {'range': [60, 80], 'color': \"lightgreen\"},\n",
    "            {'range': [80, 100], 'color': \"green\"}],\n",
    "        'threshold': {\n",
    "            'line': {'color': \"red\", 'width': 4},\n",
    "            'thickness': 0.75,\n",
    "            'value': 90}}))\n",
    "\n",
    "fig.update_layout(height=400)\n",
    "fig.show()\n",
    "\n",
    "# Display all metrics\n",
    "for metric, value in overall_metrics.items():\n",
    "    print(f\"{metric}: {value:.2f}\" if isinstance(value, float) else f\"{metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Risk Level Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Risk level distribution\n",
    "risk_counts = df['risk_level'].value_counts()\n",
    "\n",
    "# Create donut chart\n",
    "colors = {'LOW': '#2ecc71', 'MEDIUM': '#f39c12', 'HIGH': '#e74c3c', 'CRITICAL': '#c0392b'}\n",
    "fig = px.pie(values=risk_counts.values, names=risk_counts.index, \n",
    "             title='Risk Level Distribution',\n",
    "             color=risk_counts.index,\n",
    "             color_discrete_map=colors,\n",
    "             hole=0.4)\n",
    "\n",
    "fig.update_traces(textposition='inside', textinfo='percent+label')\n",
    "fig.update_layout(height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Performance by Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Language performance analysis\n",
    "lang_stats = df.groupby('language').agg({\n",
    "    'safety_score': ['mean', 'std', 'count'],\n",
    "    'compliance_score': 'mean',\n",
    "    'cultural_score': 'mean',\n",
    "    'risk_level': lambda x: (x == 'CRITICAL').sum()\n",
    "}).round(2)\n",
    "\n",
    "# Flatten column names\n",
    "lang_stats.columns = ['_'.join(col).strip() for col in lang_stats.columns.values]\n",
    "lang_stats = lang_stats.reset_index()\n",
    "\n",
    "# Create interactive bar chart\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Safety Score',\n",
    "    x=lang_stats['language'],\n",
    "    y=lang_stats['safety_score_mean'],\n",
    "    error_y=dict(type='data', array=lang_stats['safety_score_std']),\n",
    "    marker_color='lightblue'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Compliance Score',\n",
    "    x=lang_stats['language'],\n",
    "    y=lang_stats['compliance_score_mean'],\n",
    "    marker_color='lightgreen'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Cultural Score',\n",
    "    x=lang_stats['language'],\n",
    "    y=lang_stats['cultural_score_mean'],\n",
    "    marker_color='lightcoral'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Performance Scores by Language',\n",
    "    xaxis_title='Language',\n",
    "    yaxis_title='Score',\n",
    "    barmode='group',\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Display language statistics table\n",
    "print(\"\\nDetailed Language Statistics:\")\n",
    "display(lang_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Performance by Domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain performance analysis\n",
    "domain_stats = df.groupby('domain').agg({\n",
    "    'safety_score': 'mean',\n",
    "    'compliance_score': 'mean',\n",
    "    'cultural_score': 'mean',\n",
    "    'risk_score': 'mean'\n",
    "}).round(2)\n",
    "\n",
    "# Create radar chart\n",
    "categories = list(domain_stats.index)\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=domain_stats['safety_score'],\n",
    "    theta=categories,\n",
    "    fill='toself',\n",
    "    name='Safety Score'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=domain_stats['compliance_score'],\n",
    "    theta=categories,\n",
    "    fill='toself',\n",
    "    name='Compliance Score'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatterpolar(\n",
    "    r=domain_stats['cultural_score'],\n",
    "    theta=categories,\n",
    "    fill='toself',\n",
    "    name='Cultural Score'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    polar=dict(\n",
    "        radialaxis=dict(\n",
    "            visible=True,\n",
    "            range=[0, 100]\n",
    "        )),\n",
    "    showlegend=True,\n",
    "    title=\"Domain Performance Radar Chart\",\n",
    "    height=600\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Risk Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create risk heatmap\n",
    "risk_pivot = df.pivot_table(\n",
    "    values='risk_score',\n",
    "    index='language',\n",
    "    columns='domain',\n",
    "    aggfunc='mean'\n",
    ").round(2)\n",
    "\n",
    "# Create heatmap\n",
    "fig = px.imshow(\n",
    "    risk_pivot,\n",
    "    labels=dict(x=\"Domain\", y=\"Language\", color=\"Risk Score\"),\n",
    "    title=\"Risk Score Heatmap by Language and Domain\",\n",
    "    color_continuous_scale='RdYlGn_r',\n",
    "    aspect=\"auto\",\n",
    "    text_auto=True\n",
    ")\n",
    "\n",
    "fig.update_layout(height=800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Safety Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Safety score distribution\n",
    "fig = make_subplots(\n",
    "    rows=2, cols=2,\n",
    "    subplot_titles=('Overall Distribution', 'By Language', 'By Domain', 'By Risk Level'),\n",
    "    vertical_spacing=0.15,\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Overall distribution\n",
    "fig.add_trace(\n",
    "    go.Histogram(x=df['safety_score'], nbinsx=30, name='Overall'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# By language (box plot)\n",
    "for language in df['language'].unique():\n",
    "    fig.add_trace(\n",
    "        go.Box(y=df[df['language']==language]['safety_score'], name=language),\n",
    "        row=1, col=2\n",
    "    )\n",
    "\n",
    "# By domain (violin plot)\n",
    "for domain in df['domain'].unique():\n",
    "    fig.add_trace(\n",
    "        go.Violin(y=df[df['domain']==domain]['safety_score'], name=domain),\n",
    "        row=2, col=1\n",
    "    )\n",
    "\n",
    "# By risk level\n",
    "for risk_level in ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']:\n",
    "    if risk_level in df['risk_level'].values:\n",
    "        fig.add_trace(\n",
    "            go.Box(y=df[df['risk_level']==risk_level]['safety_score'], name=risk_level),\n",
    "            row=2, col=2\n",
    "        )\n",
    "\n",
    "fig.update_layout(height=1000, showlegend=False, title_text=\"Safety Score Distributions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Critical Failures Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze critical failures\n",
    "critical_failures = df[df['risk_level'] == 'CRITICAL'].copy()\n",
    "\n",
    "if len(critical_failures) > 0:\n",
    "    # Critical failures by language and domain\n",
    "    critical_pivot = critical_failures.pivot_table(\n",
    "        index='language',\n",
    "        columns='domain',\n",
    "        values='scenario_id',\n",
    "        aggfunc='count',\n",
    "        fill_value=0\n",
    "    )\n",
    "    \n",
    "    # Create heatmap of critical failures\n",
    "    fig = px.imshow(\n",
    "        critical_pivot,\n",
    "        labels=dict(x=\"Domain\", y=\"Language\", color=\"Critical Failures\"),\n",
    "        title=\"Critical Failures by Language and Domain\",\n",
    "        color_continuous_scale='Reds',\n",
    "        text_auto=True\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    # Display top critical scenarios\n",
    "    print(\"\\nTop 10 Critical Failure Scenarios:\")\n",
    "    critical_summary = critical_failures[['scenario_id', 'language', 'domain', 'risk_score']].head(10)\n",
    "    display(critical_summary)\n",
    "else:\n",
    "    print(\"No critical failures found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Model Comparison (if multiple models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if multiple models were evaluated\n",
    "if 'model' in df.columns and df['model'].nunique() > 1:\n",
    "    # Model comparison\n",
    "    model_comparison = df.groupby(['model', 'language']).agg({\n",
    "        'safety_score': 'mean'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create line chart\n",
    "    fig = px.line(\n",
    "        model_comparison,\n",
    "        x='language',\n",
    "        y='safety_score',\n",
    "        color='model',\n",
    "        title='Model Performance Comparison by Language',\n",
    "        markers=True\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=600)\n",
    "    fig.show()\n",
    "    \n",
    "    # Overall model comparison\n",
    "    model_stats = df.groupby('model').agg({\n",
    "        'safety_score': ['mean', 'std'],\n",
    "        'compliance_score': 'mean',\n",
    "        'cultural_score': 'mean',\n",
    "        'risk_level': lambda x: (x == 'CRITICAL').sum()\n",
    "    }).round(2)\n",
    "    \n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    display(model_stats)\n",
    "else:\n",
    "    print(\"Single model evaluation - no comparison available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Temporal Analysis (if timestamps available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if timestamp data is available\n",
    "if 'timestamp' in df.columns:\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    \n",
    "    # Performance over time\n",
    "    hourly_stats = df.groupby('hour').agg({\n",
    "        'safety_score': 'mean',\n",
    "        'scenario_id': 'count'\n",
    "    }).reset_index()\n",
    "    \n",
    "    # Create dual-axis chart\n",
    "    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=hourly_stats['hour'], y=hourly_stats['safety_score'],\n",
    "                   name=\"Avg Safety Score\", line=dict(color=\"blue\")),\n",
    "        secondary_y=False,\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=hourly_stats['hour'], y=hourly_stats['scenario_id'],\n",
    "               name=\"Evaluation Count\", marker_color=\"lightgray\"),\n",
    "        secondary_y=True,\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Hour of Day\")\n",
    "    fig.update_yaxes(title_text=\"Safety Score\", secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"Count\", secondary_y=True)\n",
    "    fig.update_layout(title=\"Evaluation Performance Over Time\", height=500)\n",
    "    \n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"No timestamp data available for temporal analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Analysis Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create analysis summary\n",
    "analysis_summary = {\n",
    "    'evaluation_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_evaluations': len(df),\n",
    "    'languages_tested': df['language'].unique().tolist(),\n",
    "    'domains_tested': df['domain'].unique().tolist(),\n",
    "    'overall_metrics': overall_metrics,\n",
    "    'risk_distribution': risk_counts.to_dict(),\n",
    "    'language_performance': lang_stats.to_dict('records'),\n",
    "    'domain_performance': domain_stats.to_dict(),\n",
    "    'critical_failure_count': len(critical_failures) if 'critical_failures' in locals() else 0\n",
    "}\n",
    "\n",
    "# Save summary\n",
    "import json\n",
    "with open('../results/analysis_summary.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "print(\"Analysis summary saved to ../results/analysis_summary.json\")\n",
    "print(\"\\nAnalysis complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}